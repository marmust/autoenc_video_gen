{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160b0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Model\n",
    "from decord import VideoReader, cpu\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import os, gc\n",
    "from math import floor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# merge import ahh\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63934a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "WINDOW_SIZE = 16\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "BOTTLENECK_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a4d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4065269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingFrameDataset(Dataset):\n",
    "    def __init__(self, folder_path, window_size=WINDOW_SIZE,\n",
    "                 resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
    "                 cache_dir='preprocessed_frames'):\n",
    "        self.folder_path = folder_path\n",
    "        self.window_size = window_size\n",
    "        self.resize = resize\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "        self.resize_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(resize),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.frame_files = []\n",
    "        self.index = []\n",
    "        self._prepare_frames()\n",
    "\n",
    "    def _prepare_frames(self):\n",
    "        video_files = [f for f in os.listdir(self.folder_path) if f.endswith('.mp4')]\n",
    "        for i, fname in enumerate(video_files):\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(self.cache_dir, base + '.pt')\n",
    "            if not os.path.exists(cache_path):\n",
    "                print(f'Preprocessing {fname} -> {cache_path}')\n",
    "                vr = VideoReader(os.path.join(self.folder_path, fname), ctx=cpu())\n",
    "                frames = [self.resize_transform(frame.asnumpy()) for frame in vr]\n",
    "                torch.save(torch.stack(frames), cache_path)\n",
    "                del frames, vr\n",
    "                gc.collect()\n",
    "            self.frame_files.append(cache_path)\n",
    "            frame_len = torch.load(cache_path, map_location='cpu').shape[0]\n",
    "            n_clips = floor(frame_len / self.window_size)\n",
    "            for j in range(n_clips):\n",
    "                self.index.append((i, j * self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, start = self.index[idx]\n",
    "        frames = torch.load(self.frame_files[file_idx], mmap=True, map_location='cpu')\n",
    "        return frames[start:start + self.window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455019a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=BOTTLENECK_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "\n",
    "        # Infer shape\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
    "\n",
    "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, enc_out.shape[1:]),\n",
    "            ResidualBlock(512),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.ConvTranspose2d(64, in_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.encoder_fc(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951de15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, autoenc, transformer, dataloader, epochs=EPOCHS, lr=LR, device=run_device, loss=loss):\n",
    "        self.autoenc = autoenc\n",
    "        self.transformer = transformer\n",
    "        self.dataloader = dataloader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        params = list(autoenc.parameters()) + list(transformer.parameters())\n",
    "        self.optimizer = torch.optim.Adam(params, lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
    "        self.loss_fn = loss\n",
    "\n",
    "    def train(self):\n",
    "        self.autoenc.train()\n",
    "        self.transformer.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for frames in self.dataloader:\n",
    "                frames = frames.to(self.device)\n",
    "                B, T, C, H, W = frames.shape\n",
    "                latents = self.autoenc.encode(frames.view(B * T, C, H, W)).view(B, T, -1)\n",
    "                inp = latents[:, :-1, :]\n",
    "                target_frames = frames[:, 1:, :, :, :]\n",
    "                self.optimizer.zero_grad()\n",
    "                pred_latents = self.transformer(inputs_embeds=inp).last_hidden_state\n",
    "                pred_frames = self.autoenc.decode(pred_latents.reshape(-1, BOTTLENECK_DIM)).view(B, T-1, C, H, W)\n",
    "                loss = self.loss_fn(pred_frames, target_frames)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                del frames\n",
    "                \n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f} - LR: {lr:.6f}')\n",
    "            self.scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d2f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc = ConvAutoencoder().to(run_device)\n",
    "transformer = GPT2Model.from_pretrained('decap_gpt2_cm2').to(run_device)\n",
    "\n",
    "dataset = PreprocessingFrameDataset('video_dataset')\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "trainer = Trainer(autoenc, transformer, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fd6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.0582 - LR: 0.001000\n",
      "Epoch 2/5 - Loss: 0.0406 - LR: 0.000905\n",
      "Epoch 3/5 - Loss: 0.0318 - LR: 0.000655\n",
      "Epoch 4/5 - Loss: 0.0266 - LR: 0.000345\n",
      "Epoch 5/5 - Loss: 0.0235 - LR: 0.000095\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m.save(autoenc, \u001b[33m\"\u001b[39m\u001b[33m./checkpoints/run1/autoenc\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(autoenc, \"./checkpoints/run1/autoenc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WINDOW_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mWINDOW_SIZE\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'WINDOW_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8192 / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condavnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
