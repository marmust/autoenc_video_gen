{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Model\n",
    "from decord import VideoReader, cpu\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import os, gc\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63934a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "WINDOW_SIZE = 16\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "BOTTLENECK_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg16(weights=vgg16.VGG16_Weights.IMAGENET1K_V1).features\n",
    "        self.vgg = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3'}\n",
    "        self.layer_weights = {'relu1_2': 1.0, 'relu2_2': 1.0, 'relu3_3': 1.0}\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            loss = 0.0\n",
    "            for i, layer in enumerate(self.vgg):\n",
    "                x, y = layer(x), layer(y)\n",
    "                name = self.layers.get(str(i))\n",
    "                if name:\n",
    "                    loss += self.layer_weights[name] * F.mse_loss(x, y)\n",
    "                if i > max(map(int, self.layers.keys())):\n",
    "                    break\n",
    "        return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, perceptual_weight=1.8, mse_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.perceptual_loss = PerceptualLoss().to(run_device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.mse_weight = mse_weight\n",
    "\n",
    "    def forward(self, x_recon, x_target):\n",
    "        return (self.perceptual_weight * self.perceptual_loss(x_recon, x_target)\n",
    "                + self.mse_weight * self.mse_loss(x_recon, x_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingFrameDataset(Dataset):\n",
    "    def __init__(self, folder_path, window_size=WINDOW_SIZE,\n",
    "                 resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
    "                 cache_dir='preprocessed_frames'):\n",
    "        self.folder_path = folder_path\n",
    "        self.window_size = window_size\n",
    "        self.resize = resize\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "        self.resize_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(resize),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.frame_files = []\n",
    "        self.index = []\n",
    "        self._prepare_frames()\n",
    "\n",
    "    def _prepare_frames(self):\n",
    "        video_files = [f for f in os.listdir(self.folder_path) if f.endswith('.mp4')]\n",
    "        for i, fname in enumerate(video_files):\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(self.cache_dir, base + '.pt')\n",
    "            if not os.path.exists(cache_path):\n",
    "                print(f'Preprocessing {fname} -> {cache_path}')\n",
    "                vr = VideoReader(os.path.join(self.folder_path, fname), ctx=cpu())\n",
    "                frames = [self.resize_transform(frame.asnumpy()) for frame in vr]\n",
    "                torch.save(torch.stack(frames), cache_path)\n",
    "                del frames, vr\n",
    "                gc.collect()\n",
    "            self.frame_files.append(cache_path)\n",
    "            frame_len = torch.load(cache_path, map_location='cpu').shape[0]\n",
    "            n_clips = floor(frame_len / self.window_size)\n",
    "            for j in range(n_clips):\n",
    "                self.index.append((i, j * self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, start = self.index[idx]\n",
    "        frames = torch.load(self.frame_files[file_idx], map_location='cpu')\n",
    "        return frames[start:start + self.window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455019a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=BOTTLENECK_DIM):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "        self.encoder_fc = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "            ResidualBlock(512),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.ConvTranspose2d(64, in_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.encoder_fc(x)\n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951de15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, autoenc, transformer, dataloader, epochs=EPOCHS, lr=LR, device=run_device):\n",
    "        self.autoenc = autoenc\n",
    "        self.transformer = transformer\n",
    "        self.dataloader = dataloader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        params = list(autoenc.parameters()) + list(transformer.parameters())\n",
    "        self.optimizer = torch.optim.Adam(params, lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
    "        self.loss_fn = CombinedLoss()\n",
    "\n",
    "    def train(self):\n",
    "        self.autoenc.train()\n",
    "        self.transformer.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for frames in self.dataloader:\n",
    "                frames = frames.to(self.device)\n",
    "                B, T, C, H, W = frames.shape\n",
    "                latents = self.autoenc.encode(frames.view(B*T, C, H, W)).view(B, T, -1)\n",
    "                inp = latents[:, :-1, :]\n",
    "                target_frames = frames[:, 1:, :, :, :]\n",
    "                self.optimizer.zero_grad()\n",
    "                pred_latents = self.transformer(inputs_embeds=inp).last_hidden_state\n",
    "                pred_frames = self.autoenc.decode(pred_latents.reshape(-1, BOTTLENECK_DIM)).view(B, T-1, C, H, W)\n",
    "                loss = self.loss_fn(pred_frames, target_frames)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f} - LR: {lr:.6f}')\n",
    "            self.scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc = ConvAutoencoder().to(run_device)\n",
    "transformer = GPT2Model.from_pretrained('decap_gpt2_cm2').to(run_device)\n",
    "\n",
    "dataset = PreprocessingFrameDataset('video_dataset')\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "trainer = Trainer(autoenc, transformer, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
