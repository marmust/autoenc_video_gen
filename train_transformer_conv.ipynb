{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "160b0229",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "#from torchvision.io import VideoReader\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from decord import VideoReader, cpu\n",
        "import torch.nn.functional as F\n",
        "from math import floor\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import os\n",
        "from transformers import GPT2LMHeadModel, GPT2Model\n",
        "import gc\n",
        "import json\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "63934a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# training hyperparams\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 24\n",
        "LR = 0.001\n",
        "\n",
        "# video settings\n",
        "RESOLUTION_WIDTH = 128\n",
        "RESOLUTION_HEIGHT = 128\n",
        "CHANNELS = 3\n",
        "CONVERTED_FRAMERATE = 16\n",
        "\n",
        "# model settings\n",
        "WINDOW_SIZE = 46\n",
        "ENCODED_DIM = 768\n",
        "\n",
        "# misc pytorch settings\n",
        "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TENSORBOARD_LOG_DIR = \"runs/exp7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "35a4d818",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, weights=None):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(pretrained=True).features.eval()\n",
        "\n",
        "        for layer in vgg:\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                layer.inplace = False\n",
        "\n",
        "        self.vgg = vgg\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Selected layers: low-level to mid-level features\n",
        "        self.layers = {\n",
        "            \"0\": \"relu1_1\",\n",
        "            \"3\": \"relu1_2\",\n",
        "            \"8\": \"relu2_2\",\n",
        "            \"15\": \"relu3_3\"\n",
        "        }\n",
        "\n",
        "        # Prioritize early edges more explicitly\n",
        "        self.layer_weights = weights or {\n",
        "            \"relu1_1\": 2.0,\n",
        "            \"relu1_2\": 1.5,\n",
        "            \"relu2_2\": 0.7,\n",
        "            \"relu3_3\": 0.2,\n",
        "        }\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        loss = 0.0\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            y = layer(y)\n",
        "            name = self.layers.get(str(i))\n",
        "            if name:\n",
        "                weight = self.layer_weights[name]\n",
        "                loss += weight * F.mse_loss(x, y)\n",
        "            if i > max(map(int, self.layers.keys())):\n",
        "                break\n",
        "        return loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, perceptual_weight=0.7, mse_weight=1.4):\n",
        "        super().__init__()\n",
        "        self.perceptual_loss = PerceptualLoss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.mse_weight = mse_weight\n",
        "\n",
        "    def forward(self, reconstructed_images, target_images):\n",
        "        return (\n",
        "            self.perceptual_weight * self.perceptual_loss(reconstructed_images, target_images)\n",
        "            + self.mse_weight * self.mse_loss(reconstructed_images, target_images)\n",
        "        )\n",
        "\n",
        "loss_fn = CombinedLoss()\n",
        "loss_fn = loss_fn.to(run_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "455019a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, in_channels=CHANNELS, latent_dim=ENCODED_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 4, 2, 1),  # 64x64\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),           # 32x32\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(64),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),          # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),         # 8x8\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
        "            enc_out = self.encoder(dummy)\n",
        "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
        "\n",
        "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
        "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, enc_out.shape[1:]),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x32\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(64),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x64\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # 128x128\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.tanh(self.encoder_fc(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = torch.tanh(self.decoder_fc(z))\n",
        "        z = torch.sigmoid(self.decoder(z))\n",
        "        \n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4065269f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreprocessingFrameDataset(Dataset):\n",
        "    def __init__(self, folder_path, window_size=WINDOW_SIZE,\n",
        "                 resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
        "                 framerate=CONVERTED_FRAMERATE,\n",
        "                 cache_dir='preprocessed_frames'):\n",
        "        self.folder_path = folder_path\n",
        "        self.window_size = window_size\n",
        "        self.resize = resize\n",
        "        self.framerate = framerate\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "        self.resize_transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize(resize),\n",
        "            T.ToTensor(),\n",
        "            T.Lambda(lambda x: x * 2 - 1)  # normalize [0,1] â†’ [-1,1]\n",
        "        ])\n",
        "\n",
        "        self.frame_files = []\n",
        "        self.index = []\n",
        "        self._prepare_frames()\n",
        "\n",
        "    def _prepare_frames(self):\n",
        "        video_files = [f for f in os.listdir(self.folder_path) if f.endswith('.mp4')]\n",
        "        \n",
        "        for i, fname in enumerate(video_files):\n",
        "            base = os.path.splitext(fname)[0]\n",
        "            cache_path = os.path.join(self.cache_dir, base + '.pt')\n",
        "            \n",
        "            if not os.path.exists(cache_path):\n",
        "                print(f'Preprocessing {fname} -> {cache_path}')\n",
        "                vr = VideoReader(os.path.join(self.folder_path, fname), ctx=cpu())\n",
        "                original_fps = vr.get_avg_fps()\n",
        "                step = max(int(original_fps // self.framerate), 1)\n",
        "\n",
        "                frame_indices = list(range(0, len(vr), step))\n",
        "                n_frames = len(frame_indices)\n",
        "\n",
        "                # Preallocate tensor for all resized frames (C, H, W)\n",
        "                sample_frame = self.resize_transform(vr[0].asnumpy())  # to get shape\n",
        "                C, H, W = sample_frame.shape\n",
        "                frame_tensor = torch.empty((n_frames, C, H, W), dtype=sample_frame.dtype)\n",
        "\n",
        "                for idx, frame_idx in enumerate(frame_indices):\n",
        "                    frame_tensor[idx] = self.resize_transform(vr[frame_idx].asnumpy())\n",
        "\n",
        "                torch.save(frame_tensor, cache_path)\n",
        "                del frame_tensor, vr\n",
        "                gc.collect()\n",
        "            \n",
        "            self.frame_files.append(cache_path)\n",
        "            frame_len = torch.load(cache_path, map_location='cpu').shape[0]\n",
        "            n_clips = floor(frame_len / self.window_size)\n",
        "            for j in range(n_clips):\n",
        "                self.index.append((i, j * self.window_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx, start = self.index[idx]\n",
        "        frames = torch.load(self.frame_files[file_idx], mmap=True, map_location='cpu')\n",
        "        return frames[start:start + self.window_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "951de15b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, autoencoder, transformer, dataloader, RESOLUTION_HEIGHT=RESOLUTION_HEIGHT, RESOLUTION_WIDTH=RESOLUTION_WIDTH, BOTTLENECK_DIM=ENCODED_DIM, epochs=EPOCHS, lr=LR, device=run_device, loss=loss_fn, writer: SummaryWriter = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR)):\n",
        "        self.autoencoder = autoencoder\n",
        "        self.transformer = transformer\n",
        "        self.dataloader = dataloader\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.writer = writer\n",
        "        params = list(autoencoder.parameters()) + list(transformer.parameters())\n",
        "        self.optimizer = torch.optim.Adam(params, lr=lr)\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
        "        self.loss_fn = loss\n",
        "        \n",
        "        self.RESOLUTION_HEIGHT = RESOLUTION_HEIGHT\n",
        "        self.RESOLUTION_WIDTH = RESOLUTION_WIDTH\n",
        "        self.BOTTLENECK_DIM = BOTTLENECK_DIM\n",
        "\n",
        "    def train(self):\n",
        "        self.autoencoder.train()\n",
        "        self.transformer.train()\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0.0\n",
        "            \n",
        "            for batch in self.dataloader:\n",
        "                # !!! WARNING !!! the following segment was revealed to me in a dream !!! DO NOT MODIFY !!!\n",
        "                \n",
        "                batch = batch.to(self.device)\n",
        "                B, T, C, H, W = batch.shape\n",
        "                \n",
        "                # split the frames into inputs and outputs (shifted by 1 futureward)\n",
        "                input_frames = batch[:, :-1, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE PAST]\n",
        "                output_frames = batch[:, 1:, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE FUTURE]\n",
        "                \n",
        "                # encode the WHOLE sequence even across batches\n",
        "                #                  in:   (B, T-1, C, H, W)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
        "                input_latents = self.autoencoder.encode(input_frames.view(B * (T - 1), C, H, W)).view(B, T - 1, self.BOTTLENECK_DIM)\n",
        "                \n",
        "                # run the latents thru the transformer\n",
        "                #                          [1 TOWARDS THE PAST]                           [1 TOWARDS THE FUTURE]\n",
        "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
        "                predicted_latents = self.transformer(inputs_embeds=input_latents).last_hidden_state\n",
        "                \n",
        "                # decode the predicted future back to frames\n",
        "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, C, H, W)\n",
        "                predicted_frames = self.autoencoder.decode(predicted_latents.reshape(-1, self.BOTTLENECK_DIM)).view(B, T - 1, C, H, W)\n",
        "                \n",
        "                # calculate the loss between the predicted frames and the target frames\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                # VGG loss CANNOT handler a time dim, so we combine the sequences with the batches tto trick VGG into thinking that its only batches\n",
        "                # NOTE: this doesnt cause cross batch contamination since view works the EXACT same way twice, aligning each target frame with its corresponding prediction\n",
        "                predicted_frames = predicted_frames.view(-1, C, H, W)  # (B * (T-1), C, H, W)\n",
        "                output_frames = output_frames.view(-1, C, H, W)        # (B * (T-1), C, H, W)\n",
        "                \n",
        "                loss = self.loss_fn(predicted_frames, output_frames)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            # just for the video logging, reshape back to a sequence format (with batches)\n",
        "            output_frames_video = output_frames.view(B, T - 1, C, H, W) / 2 + 0.5 # normalize [-1,1] â†’ [0,1]\n",
        "            predicted_frames_video = predicted_frames.view(B, T - 1, C, H, W) / 2 + 0.5 # normalize [-1,1] â†’ [0,1]\n",
        "            \n",
        "            # logging\n",
        "            avg_loss = total_loss / len(self.dataloader)\n",
        "            lr = self.optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f} - LR: {lr:.6f}')\n",
        "            self.writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
        "            self.writer.add_scalar(\"LearningRate\", lr, epoch)\n",
        "            self.scheduler.step()\n",
        "            \n",
        "            self.writer.add_video(\"expected_output\", output_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
        "            self.writer.add_video(\"transformer_output\", predicted_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
        "        \n",
        "        self.writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load GPT2 and decapitate\n",
        "gpt2_full = GPT2LMHeadModel.from_pretrained(\"gpt2\", device_map=run_device)\n",
        "decap_gpt2 = gpt2_full.transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standart autoenc initialization here\n",
        "autoencoder = ConvAutoencoder().to(run_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "42d2f22f",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = PreprocessingFrameDataset('video_dataset')\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "\n",
        "trainer = Trainer(autoencoder, decap_gpt2, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fd6fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(autoencoder.state_dict(), \"checkpoints/run7/autoenc.pth\")\n",
        "gpt2_full.save_pretrained(\"checkpoints/run7/gpt2_decap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "condavnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
