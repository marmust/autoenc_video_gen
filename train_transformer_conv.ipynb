{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "160b0229",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from decord import VideoReader, cpu\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "from math import floor\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import os\n",
        "from transformers import GPT2LMHeadModel, GPT2Model\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63934a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# training hyperparams\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 24\n",
        "LR = 0.001\n",
        "\n",
        "# video settings\n",
        "RESOLUTION_WIDTH = 128\n",
        "RESOLUTION_HEIGHT = 128\n",
        "CHANNELS = 3\n",
        "CONVERTED_FRAMERATE = 16\n",
        "\n",
        "# model settings\n",
        "WINDOW_SIZE = 46\n",
        "ENCODED_DIM = 768\n",
        "\n",
        "# misc pytorch settings\n",
        "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TENSORBOARD_LOG_DIR = \"runs/transformer_conv/exp17\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a4d818",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, weights=None):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(pretrained=True).features.eval()\n",
        "\n",
        "        for layer in vgg:\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                layer.inplace = False\n",
        "\n",
        "        self.vgg = vgg\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Selected layers: low-level to mid-level features\n",
        "        self.layers = {\n",
        "            \"0\": \"relu1_1\",\n",
        "            \"3\": \"relu1_2\",\n",
        "            \"8\": \"relu2_2\",\n",
        "            \"15\": \"relu3_3\"\n",
        "        }\n",
        "\n",
        "        # Prioritize early edges more explicitly\n",
        "        self.layer_weights = weights or {\n",
        "            \"relu1_1\": 2.0,\n",
        "            \"relu1_2\": 1.5,\n",
        "            \"relu2_2\": 0.7,\n",
        "            \"relu3_3\": 0.2,\n",
        "        }\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        loss = 0.0\n",
        "        for i, layer in enumerate(self.vgg):\n",
        "            x = layer(x)\n",
        "            y = layer(y)\n",
        "            name = self.layers.get(str(i))\n",
        "            if name:\n",
        "                weight = self.layer_weights[name]\n",
        "                loss += weight * F.mse_loss(x, y)\n",
        "            if i > max(map(int, self.layers.keys())):\n",
        "                break\n",
        "        return loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, perceptual_weight=0.7, mse_weight=1.4):\n",
        "        super().__init__()\n",
        "        self.perceptual_loss = PerceptualLoss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.mse_weight = mse_weight\n",
        "\n",
        "    def forward(self, reconstructed_images, target_images):\n",
        "        return (\n",
        "            self.perceptual_weight * self.perceptual_loss(reconstructed_images, target_images)\n",
        "            + self.mse_weight * self.mse_loss(reconstructed_images, target_images)\n",
        "        )\n",
        "\n",
        "loss_fn = CombinedLoss()\n",
        "loss_fn = loss_fn.to(run_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "455019a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, in_channels=CHANNELS, latent_dim=ENCODED_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 4, 2, 1),  # 64x64\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),           # 32x32\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(64),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),          # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),         # 8x8\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
        "            enc_out = self.encoder(dummy)\n",
        "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
        "\n",
        "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
        "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, enc_out.shape[1:]),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x32\n",
        "            nn.ReLU(),\n",
        "            ResidualBlock(64),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x64\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # 128x128\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.encoder_fc(x))\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = torch.relu(self.decoder_fc(z))\n",
        "        z = self.decoder(z)\n",
        "        \n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4065269f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreprocessingFrameDataset(Dataset):\n",
        "    def __init__(self, folder_path, window_size=WINDOW_SIZE,\n",
        "                 resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
        "                 framerate=CONVERTED_FRAMERATE,\n",
        "                 cache_dir='preprocessed_frames'):\n",
        "        self.folder_path = folder_path\n",
        "        self.window_size = window_size\n",
        "        self.resize = resize\n",
        "        self.framerate = framerate\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "        self.resize_transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize(resize),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.frame_files = []\n",
        "        self.index = []\n",
        "        self._prepare_frames()\n",
        "\n",
        "    def _prepare_frames(self):\n",
        "        video_files = [f for f in os.listdir(self.folder_path) if f.endswith('.mp4')]\n",
        "        for i, fname in enumerate(video_files):\n",
        "            base = os.path.splitext(fname)[0]\n",
        "            cache_path = os.path.join(self.cache_dir, base + '.pt')\n",
        "            if not os.path.exists(cache_path):\n",
        "                print(f'Preprocessing {fname} -> {cache_path}')\n",
        "                vr = VideoReader(os.path.join(self.folder_path, fname), ctx=cpu())\n",
        "                original_fps = vr.get_avg_fps()\n",
        "                step = max(int(original_fps // self.framerate), 1)\n",
        "\n",
        "                # sample frames uniformly based on framerate\n",
        "                frame_indices = list(range(0, len(vr), step))\n",
        "                frames = [self.resize_transform(vr[i].asnumpy()) for i in frame_indices]\n",
        "                torch.save(torch.stack(frames), cache_path)\n",
        "                del frames, vr\n",
        "                gc.collect()\n",
        "\n",
        "            self.frame_files.append(cache_path)\n",
        "            frame_len = torch.load(cache_path, map_location='cpu').shape[0]\n",
        "            n_clips = floor(frame_len / self.window_size)\n",
        "            for j in range(n_clips):\n",
        "                self.index.append((i, j * self.window_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx, start = self.index[idx]\n",
        "        frames = torch.load(self.frame_files[file_idx], mmap=True, map_location='cpu')\n",
        "        return frames[start:start + self.window_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "951de15b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, autoencoder, transformer, dataloader, RESOLUTION_HEIGHT=RESOLUTION_HEIGHT, RESOLUTION_WIDTH=RESOLUTION_WIDTH, BOTTLENECK_DIM=ENCODED_DIM, epochs=EPOCHS, lr=LR, device=run_device, loss=loss_fn, writer: SummaryWriter = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR)):\n",
        "        self.autoencoder = autoencoder\n",
        "        self.transformer = transformer\n",
        "        self.dataloader = dataloader\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.writer = writer\n",
        "        params = list(autoencoder.parameters()) + list(transformer.parameters())\n",
        "        self.optimizer = torch.optim.Adam(params, lr=lr)\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
        "        self.loss_fn = loss\n",
        "        \n",
        "        self.RESOLUTION_HEIGHT = RESOLUTION_HEIGHT\n",
        "        self.RESOLUTION_WIDTH = RESOLUTION_WIDTH\n",
        "        self.BOTTLENECK_DIM = BOTTLENECK_DIM\n",
        "\n",
        "    def train(self):\n",
        "        self.autoencoder.train()\n",
        "        self.transformer.train()\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0.0\n",
        "            \n",
        "            for batch in self.dataloader:\n",
        "                # !!! WARNING !!! the following segment was revealed to me in a dream !!! DO NOT MODIFY !!!\n",
        "                \n",
        "                batch = batch.to(self.device)\n",
        "                B, T, C, H, W = batch.shape\n",
        "                \n",
        "                # split the frames into inputs and outputs (shifted by 1 futureward)\n",
        "                input_frames = batch[:, :-1, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE PAST]\n",
        "                output_frames = batch[:, 1:, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE FUTURE]\n",
        "                \n",
        "                # encode the WHOLE sequence even across batches\n",
        "                #                  in:   (B, T-1, C, H, W)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
        "                input_latents = self.autoencoder.encode(input_frames.view(B * (T - 1), C, H, W)).view(B, T - 1, self.BOTTLENECK_DIM)\n",
        "                \n",
        "                # run the latents thru the transformer\n",
        "                #                          [1 TOWARDS THE PAST]                           [1 TOWARDS THE FUTURE]\n",
        "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
        "                predicted_latents = self.transformer(inputs_embeds=input_latents).last_hidden_state\n",
        "                \n",
        "                # decode the predicted future back to frames\n",
        "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, C, H, W)\n",
        "                predicted_frames = self.autoencoder.decode(predicted_latents.reshape(-1, self.BOTTLENECK_DIM)).view(B, T - 1, C, H, W)\n",
        "                \n",
        "                # calculate the loss between the predicted frames and the target frames\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                # VGG loss CANNOT handler a time dim, so we combine the sequences with the batches tto trick VGG into thinking that its only batches\n",
        "                # NOTE: this doesnt cause cross batch contamination since view works the EXACT same way twice, aligning each target frame with its corresponding prediction\n",
        "                predicted_frames = predicted_frames.view(-1, C, H, W)  # (B * (T-1), C, H, W)\n",
        "                output_frames = output_frames.view(-1, C, H, W)        # (B * (T-1), C, H, W)\n",
        "                \n",
        "                loss = self.loss_fn(predicted_frames, output_frames)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            # just for the video logging, reshape back to a sequence format (with batches)\n",
        "            output_frames_video = output_frames.view(B, T - 1, C, H, W)\n",
        "            predicted_frames_video = predicted_frames.view(B, T - 1, C, H, W)\n",
        "            \n",
        "            # logging\n",
        "            avg_loss = total_loss / len(self.dataloader)\n",
        "            lr = self.optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f} - LR: {lr:.6f}')\n",
        "            self.writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
        "            self.writer.add_scalar(\"LearningRate\", lr, epoch)\n",
        "            self.scheduler.step()\n",
        "            \n",
        "            self.writer.add_video(\"expected_output\", output_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
        "            self.writer.add_video(\"transformer_output\", predicted_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
        "        \n",
        "        self.writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load GPT2 and decapitate\n",
        "gpt2_full = GPT2LMHeadModel.from_pretrained(\"gpt2\", device_map=run_device)\n",
        "decap_gpt2 = gpt2_full.transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standart autoenc initialization here\n",
        "autoencoder = ConvAutoencoder().to(run_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "42d2f22f",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = PreprocessingFrameDataset('video_dataset')\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "\n",
        "trainer = Trainer(autoencoder, decap_gpt2, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fd6fc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/24 - Loss: 0.1516 - LR: 0.001000\n",
            "Epoch 2/24 - Loss: 0.1128 - LR: 0.000996\n",
            "Epoch 3/24 - Loss: 0.1087 - LR: 0.000983\n",
            "Epoch 4/24 - Loss: 0.1064 - LR: 0.000962\n",
            "Epoch 5/24 - Loss: 0.1048 - LR: 0.000933\n",
            "Epoch 6/24 - Loss: 0.1037 - LR: 0.000897\n",
            "Epoch 7/24 - Loss: 0.1024 - LR: 0.000854\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory checkpoints/run15 does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints/run15/autoenc.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m gpt2_full.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints/run15/gpt2_decap\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\serialization.py:964\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    961\u001b[39m     f = os.fspath(f)\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    965\u001b[39m         _save(\n\u001b[32m    966\u001b[39m             obj,\n\u001b[32m    967\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m             _disable_byteorder_record,\n\u001b[32m    971\u001b[39m         )\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
            "\u001b[31mRuntimeError\u001b[39m: Parent directory checkpoints/run15 does not exist."
          ]
        }
      ],
      "source": [
        "torch.save(autoencoder.state_dict(), \"checkpoints/run15/autoenc.pth\")\n",
        "gpt2_full.save_pretrained(\"checkpoints/run15/gpt2_decap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "condavnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
