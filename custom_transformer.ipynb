{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from decord import VideoReader, cpu\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Model\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 256\n",
    "LR = 0.001\n",
    "\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "CONVERTED_FRAMERATE = 20\n",
    "\n",
    "WINDOW_SIZE = 28\n",
    "INPUT_WINDOW_SIZE = WINDOW_SIZE - 1\n",
    "ENCODED_DIM = 768\n",
    "NUM_TRANSFORMER_BLOCKS = 12\n",
    "MLP_HIDDEN_DIM = 3072\n",
    "NUM_HEADS = 12\n",
    "\n",
    "DROPOUT = 0.0\n",
    "\n",
    "TENSORBOARD_LOG_DIR = \"runs/custom_transformer/exp1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOSS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "        vgg = vgg16(pretrained=True).features.eval()\n",
    "\n",
    "        # Disable inplace ReLUs to avoid autograd bugs\n",
    "        for layer in vgg:\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                layer.inplace = False\n",
    "\n",
    "        self.vgg = vgg\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.layers = {\n",
    "            \"0\": \"relu1_1\",\n",
    "            \"3\": \"relu1_2\",\n",
    "            \"8\": \"relu2_2\",\n",
    "            \"15\": \"relu3_3\"\n",
    "        }\n",
    "\n",
    "        self.layer_weights = weights or {\n",
    "            \"relu1_1\": 1.5,\n",
    "            \"relu1_2\": 1.0,\n",
    "            \"relu2_2\": 0.8,\n",
    "            \"relu3_3\": 0.3,\n",
    "        }\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = 0.0\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            name = self.layers.get(str(i))\n",
    "            if name:\n",
    "                loss += self.layer_weights[name] * F.mse_loss(x, y)\n",
    "            if i > max(map(int, self.layers.keys())):\n",
    "                break\n",
    "        return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, perceptual_weight=1.8, mse_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.mse_weight = mse_weight\n",
    "\n",
    "    def forward(self, reconstructed_images, target_images):\n",
    "        return (\n",
    "            self.perceptual_weight * self.perceptual_loss(reconstructed_images, target_images)\n",
    "            + self.mse_weight * self.mse_loss(reconstructed_images, target_images)\n",
    "        )\n",
    "\n",
    "loss_fn = CombinedLoss()\n",
    "loss_fn = loss_fn.to(run_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRANSFORMER ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert ENCODED_DIM % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = ENCODED_DIM // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(ENCODED_DIM, 3 * ENCODED_DIM)\n",
    "        self.out_proj = nn.Linear(ENCODED_DIM, ENCODED_DIM)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, T, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if attn_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, mlp_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.attn = CausalSelfAttention(num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(ENCODED_DIM, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, ENCODED_DIM),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln1(x), attn_mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len=INPUT_WINDOW_SIZE, num_heads=NUM_HEADS, mlp_hidden_dim=MLP_HIDDEN_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(NUM_TRANSFORMER_BLOCKS)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        attn_mask = self.causal_mask[:, :, :T, :T]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask)\n",
    "\n",
    "        return self.ln_f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AUTOENCODER ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=ENCODED_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 4, 2, 1),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),           # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),          # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),         # 8x8\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
    "\n",
    "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, enc_out.shape[1:]),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.encoder_fc(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MISC ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingFrameDataset(Dataset):\n",
    "    def __init__(self, folder_path, window_size=WINDOW_SIZE,\n",
    "                 resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
    "                 framerate=CONVERTED_FRAMERATE,\n",
    "                 cache_dir='preprocessed_frames'):\n",
    "        self.folder_path = folder_path\n",
    "        self.window_size = window_size\n",
    "        self.resize = resize\n",
    "        self.framerate = framerate\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "        self.resize_transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(resize),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.frame_files = []\n",
    "        self.index = []\n",
    "        self._prepare_frames()\n",
    "\n",
    "    def _prepare_frames(self):\n",
    "        video_files = [f for f in os.listdir(self.folder_path) if f.endswith('.mp4')]\n",
    "        for i, fname in enumerate(video_files):\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            cache_path = os.path.join(self.cache_dir, base + '.pt')\n",
    "            if not os.path.exists(cache_path):\n",
    "                print(f'Preprocessing {fname} -> {cache_path}')\n",
    "                vr = VideoReader(os.path.join(self.folder_path, fname), ctx=cpu())\n",
    "                original_fps = vr.get_avg_fps()\n",
    "                step = max(int(original_fps // self.framerate), 1)\n",
    "\n",
    "                # sample frames uniformly based on framerate\n",
    "                frame_indices = list(range(0, len(vr), step))\n",
    "                frames = [self.resize_transform(vr[i].asnumpy()) for i in frame_indices]\n",
    "                torch.save(torch.stack(frames), cache_path)\n",
    "                del frames, vr\n",
    "                gc.collect()\n",
    "\n",
    "            self.frame_files.append(cache_path)\n",
    "            frame_len = torch.load(cache_path, map_location='cpu').shape[0]\n",
    "            n_clips = floor(frame_len / self.window_size)\n",
    "            for j in range(n_clips):\n",
    "                self.index.append((i, j * self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, start = self.index[idx]\n",
    "        frames = torch.load(self.frame_files[file_idx], mmap=True, map_location='cpu')\n",
    "        return frames[start:start + self.window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, autoenc, transformer, dataloader, RESOLUTION_HEIGHT=RESOLUTION_HEIGHT, RESOLUTION_WIDTH=RESOLUTION_WIDTH, BOTTLENECK_DIM=ENCODED_DIM, epochs=EPOCHS, lr=LR, device=run_device, loss=loss_fn, writer: SummaryWriter = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR)):\n",
    "        self.autoenc = autoenc\n",
    "        self.transformer = transformer\n",
    "        self.dataloader = dataloader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.writer = writer\n",
    "        params = list(autoenc.parameters()) + list(transformer.parameters())\n",
    "        self.optimizer = torch.optim.Adam(params, lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=epochs)\n",
    "        self.loss_fn = loss\n",
    "        \n",
    "        self.RESOLUTION_HEIGHT = RESOLUTION_HEIGHT\n",
    "        self.RESOLUTION_WIDTH = RESOLUTION_WIDTH\n",
    "        self.BOTTLENECK_DIM = BOTTLENECK_DIM\n",
    "\n",
    "    def train(self):\n",
    "        self.autoenc.train()\n",
    "        self.transformer.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for batch in self.dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                B, T, C, H, W = batch.shape\n",
    "                \n",
    "                # split the frames into inputs and outputs (shifted by 1 futureward)\n",
    "                input_frames = batch[:, :-1, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE PAST]\n",
    "                output_frames = batch[:, 1:, :, :, :].clone()                    # (B, T-1, C, H, W)     [1 TOWARDS THE FUTURE]\n",
    "                \n",
    "                # encode the WHOLE sequence even across batches\n",
    "                #                  in:   (B, T-1, C, H, W)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
    "                input_latents = self.autoenc.encode(input_frames.view(B * (T - 1), C, H, W)).view(B, T - 1, self.BOTTLENECK_DIM)\n",
    "                \n",
    "                # run the latents thru the transformer\n",
    "                #                          [1 TOWARDS THE PAST]                           [1 TOWARDS THE FUTURE]\n",
    "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, BOTTLENECK_DIM)\n",
    "                predicted_latents = self.transformer(inputs_embeds=input_latents).last_hidden_state\n",
    "                \n",
    "                # decode the predicted future back to frames\n",
    "                #                  in:   (B, T-1, BOTTLENECK_DIM)     ------>     out:   (B, T-1, C, H, W)\n",
    "                predicted_frames = self.autoenc.decode(predicted_latents.reshape(-1, self.BOTTLENECK_DIM)).view(B, T - 1, C, H, W)\n",
    "                \n",
    "                # calculate the loss between the predicted frames and the target frames\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # VGG loss CANNOT handler a time dim, so we combine the sequences with the batches tto trick VGG into thinking that its only batches\n",
    "                # NOTE: this doesnt cause cross batch contamination since view works the EXACT same way twice, aligning each target frame with its corresponding prediction\n",
    "                predicted_frames = predicted_frames.view(-1, C, H, W)  # (B * (T-1), C, H, W)\n",
    "                output_frames = output_frames.view(-1, C, H, W)        # (B * (T-1), C, H, W)\n",
    "                \n",
    "                loss = self.loss_fn(predicted_frames, output_frames)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # just for the video logging, reshape back to a sequence format (with batches)\n",
    "            output_frames_video = output_frames.view(B, T - 1, C, H, W)\n",
    "            predicted_frames_video = predicted_frames.view(B, T - 1, C, H, W)\n",
    "\n",
    "            \n",
    "            # logging\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.4f} - LR: {lr:.6f}')\n",
    "            self.writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "            self.writer.add_scalar(\"LearningRate\", lr, epoch)\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            self.writer.add_video(\"expected_output\", output_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
    "            self.writer.add_video(\"transformer_output\", predicted_frames_video, global_step=epoch, fps=CONVERTED_FRAMERATE)\n",
    "        \n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder = autoencoder.to(run_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer()\n",
    "transformer = transformer.to(run_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreprocessingFrameDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mPreprocessingFrameDataset\u001b[49m(folder_path=\u001b[33m\"\u001b[39m\u001b[33mvideo_dataset\u001b[39m\u001b[33m\"\u001b[39m, window_size=WINDOW_SIZE, resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT), framerate=CONVERTED_FRAMERATE)\n\u001b[32m      2\u001b[39m dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m trainer = Trainer(autoencoder, transformer, dataloader)\n",
      "\u001b[31mNameError\u001b[39m: name 'PreprocessingFrameDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = PreprocessingFrameDataset(folder_path=\"video_dataset\", window_size=WINDOW_SIZE, resize=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT), framerate=CONVERTED_FRAMERATE)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "trainer = Trainer(autoencoder, transformer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condavnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
