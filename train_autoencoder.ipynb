{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c96b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9155dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# pytorch settings\n",
    "run_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# video settings\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "\n",
    "# model settings\n",
    "BOTTLENECK_DIM = 28*28\n",
    "\n",
    "# training settings\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 512\n",
    "optim = torch.optim.AdamW\n",
    "lr = 0.001\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg16(pretrained=True).features.eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.layers = {\n",
    "            \"0\": \"relu1_1\",\n",
    "            \"3\": \"relu1_2\",\n",
    "            \"8\": \"relu2_2\",\n",
    "            \"15\": \"relu3_3\"\n",
    "        }\n",
    "\n",
    "        self.layer_weights = weights or {\n",
    "            \"relu1_1\": 1.5,\n",
    "            \"relu1_2\": 1.0,\n",
    "            \"relu2_2\": 0.8,\n",
    "            \"relu3_3\": 0.3,\n",
    "        }\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            loss = 0.0\n",
    "            for i, layer in enumerate(self.vgg):\n",
    "                x, y = layer(x), layer(y)\n",
    "                name = self.layers.get(str(i))\n",
    "                if name:\n",
    "                    loss += self.layer_weights[name] * F.mse_loss(x, y)\n",
    "                if i > max(map(int, self.layers.keys())):\n",
    "                    break\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, perceptual_weight=1.8, mse_weight=0.2, latent_weight=0.0):\n",
    "        super().__init__()\n",
    "        self.perceptual_loss = PerceptualLoss().to(run_device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.latent_weight = latent_weight\n",
    "\n",
    "    def forward(self, x_recon, x_target, z=None, z_target=None):\n",
    "        loss = (\n",
    "            self.perceptual_weight * self.perceptual_loss(x_recon, x_target)\n",
    "            + self.mse_weight * self.mse_loss(x_recon, x_target)\n",
    "        )\n",
    "        if self.latent_weight > 0 and z is not None and z_target is not None:\n",
    "            loss += self.latent_weight * F.mse_loss(z, z_target)\n",
    "        return loss\n",
    "\n",
    "loss = CombinedLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "\n",
    "# tensorboard settings\n",
    "TENSORBOARD_LOG_DIR = \"runs/autoencoder/exp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b7d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def tensor_to_pil(self, image_tensor: torch.Tensor) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert a tensor to a PIL Image.\n",
    "        \n",
    "        Args:\n",
    "            image_tensor (torch.Tensor): A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \n",
    "        Returns:\n",
    "            Image.Image: A PIL Image object.\n",
    "        \"\"\"\n",
    "        # Clamp to [0, 1], convert to [0, 255] and uint8\n",
    "        image_np = (image_tensor.clamp(0, 1).mul(255).byte().cpu().permute(1, 2, 0).numpy())\n",
    "        return Image.fromarray(image_np)\n",
    "    \n",
    "    def pil_to_tensor(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a PIL image to a PyTorch tensor of shape (C, H, W) with values in [0, 1].\n",
    "        \n",
    "        Args:\n",
    "            image (Image.Image): A PIL Image object.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \"\"\"\n",
    "        return transforms.ToTensor()(image)  # Already returns (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075ccba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator:\n",
    "    def __init__(self, folder_path: str):\n",
    "        self.images = [Image.open(os.path.join(folder_path, fname)).convert(\"RGBA\") for fname in os.listdir(folder_path) if fname.lower().endswith(\".png\")]\n",
    "    \n",
    "    def generate_random_image(\n",
    "        self,\n",
    "        resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT),\n",
    "        num_layers_range=(2, 24),\n",
    "        scale_fraction_range=(0.15, 1.2),  # as % of canvas dimensions\n",
    "        allow_rotation=True,\n",
    "        apply_color_tint=True\n",
    "    ) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Generate a composite RGBA image with random base color and transformed overlays.\n",
    "        \n",
    "        Args:\n",
    "            resolution (tuple): Output image size (width, height).\n",
    "            num_layers_range (tuple): Min and max number of overlays to paste.\n",
    "            scale_fraction_range (tuple): Min and max fraction of resolution to scale overlays.\n",
    "            allow_rotation (bool): Whether to apply random rotation to overlays.\n",
    "            apply_color_tint (bool): Whether to apply random color tints.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Final composited RGBA image.\n",
    "        \"\"\"\n",
    "        # Create base RGBA image with a random opaque color\n",
    "        base_color = tuple(random.randint(0, 255) for _ in range(3)) + (255,)\n",
    "        base = Image.new(\"RGBA\", resolution, base_color)\n",
    "\n",
    "        num_layers = random.randint(*num_layers_range)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            overlay = random.choice(self.images).copy().convert(\"RGBA\")\n",
    "\n",
    "            # Random scaling based on canvas size\n",
    "            scale_w = random.uniform(*scale_fraction_range)\n",
    "            scale_h = random.uniform(*scale_fraction_range)\n",
    "            new_size = (\n",
    "                int(resolution[0] * scale_w),\n",
    "                int(resolution[1] * scale_h)\n",
    "            )\n",
    "            overlay = overlay.resize(new_size, resample=Image.BICUBIC)\n",
    "\n",
    "            # Optional rotation\n",
    "            \n",
    "            if allow_rotation:\n",
    "                angle = random.uniform(0, 360)\n",
    "                overlay = overlay.rotate(angle, expand=True)\n",
    "\n",
    "            # Optional color tint (multiply RGB channels)\n",
    "            if apply_color_tint:\n",
    "                r, g, b, a = overlay.split()\n",
    "                tint_factors = [random.uniform(0.25, 1.0) for _ in range(3)]\n",
    "                r = r.point(lambda i: int(i * tint_factors[0]))\n",
    "                g = g.point(lambda i: int(i * tint_factors[1]))\n",
    "                b = b.point(lambda i: int(i * tint_factors[2]))\n",
    "                \n",
    "                overlay = Image.merge(\"RGBA\", (r, g, b, a))\n",
    "\n",
    "            # Allow pasting outside canvas\n",
    "            offset_x = random.randint(-overlay.size[0] // 2, resolution[0])\n",
    "            offset_y = random.randint(-overlay.size[1] // 2, resolution[1])\n",
    "            pos = (offset_x, offset_y)\n",
    "\n",
    "            # Create transparent layer and paste overlay\n",
    "            temp_layer = Image.new(\"RGBA\", resolution, (0, 0, 0, 0))\n",
    "            temp_layer.paste(overlay, pos, overlay)\n",
    "\n",
    "            # Composite with base\n",
    "            base = Image.alpha_composite(base, temp_layer)\n",
    "\n",
    "        return base.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTargetLatentCompressor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A reference compressor that encodes RGB images into a fixed-size 28×28 grayscale latent\n",
    "        by squeezing R, G, B channels side-by-side horizontally, with any remaining space padded.\n",
    "        \"\"\"\n",
    "        self.encoding_size = int(BOTTLENECK_DIM ** 0.5)\n",
    "        self.channel_width = self.encoding_size // CHANNELS\n",
    "        self.padding_width = self.encoding_size - (self.channel_width * CHANNELS)\n",
    "\n",
    "    def image_to_latent(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compress an RGB image into a 28×28 grayscale latent.\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image): Input RGB image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (1, 1, 28, 28), grayscale stacked channel encoding.\n",
    "        \"\"\"\n",
    "        image = image.resize((RESOLUTION_WIDTH, RESOLUTION_HEIGHT)).convert(\"RGB\")\n",
    "        tensor = transforms.ToTensor()(image).unsqueeze(0)  # (1, 3, H, W)\n",
    "\n",
    "        r = tensor[:, 0:1]\n",
    "        g = tensor[:, 1:2]\n",
    "        b = tensor[:, 2:3]\n",
    "\n",
    "        r_comp = F.interpolate(r, size=(self.encoding_size, self.channel_width), mode='bilinear', align_corners=False)\n",
    "        g_comp = F.interpolate(g, size=(self.encoding_size, self.channel_width), mode='bilinear', align_corners=False)\n",
    "        b_comp = F.interpolate(b, size=(self.encoding_size, self.channel_width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        components = [r_comp, g_comp, b_comp]\n",
    "\n",
    "        if self.padding_width > 0:\n",
    "            pad = torch.zeros((1, 1, self.encoding_size, self.padding_width), dtype=r_comp.dtype)\n",
    "            components.append(pad)\n",
    "\n",
    "        encoded = torch.cat(components, dim=3)  # (1, 1, 28, 28)\n",
    "        return encoded\n",
    "\n",
    "    def latent_to_image(self, latent: torch.Tensor) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Decode a 28×28 latent into a full-resolution RGB image.\n",
    "\n",
    "        Args:\n",
    "            latent (torch.Tensor): Shape (1, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: RGB image of size (128, 128)\n",
    "        \"\"\"\n",
    "        assert latent.shape == (1, 1, self.encoding_size, self.encoding_size), f\"Expected latent shape (1, 1, {self.encoding_size}, {self.encoding_size})\"\n",
    "\n",
    "        r_slice = latent[:, :, :, 0:self.channel_width]\n",
    "        g_slice = latent[:, :, :, self.channel_width:self.channel_width * 2]\n",
    "        b_slice = latent[:, :, :, self.channel_width * 2:self.channel_width * 3]\n",
    "\n",
    "        r_up = F.interpolate(r_slice, size=(RESOLUTION_HEIGHT, RESOLUTION_WIDTH), mode='bilinear', align_corners=False)\n",
    "        g_up = F.interpolate(g_slice, size=(RESOLUTION_HEIGHT, RESOLUTION_WIDTH), mode='bilinear', align_corners=False)\n",
    "        b_up = F.interpolate(b_slice, size=(RESOLUTION_HEIGHT, RESOLUTION_WIDTH), mode='bilinear', align_corners=False)\n",
    "\n",
    "        reconstructed = torch.cat([r_up, g_up, b_up], dim=1)  # (1, 3, H, W)\n",
    "        reconstructed_image = transforms.ToPILImage()(reconstructed.squeeze(0).clamp(0, 1))\n",
    "        return reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681a3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomImageDataset(Dataset):\n",
    "    def __init__(self, folder_path: str, length: int):\n",
    "        self.length = length\n",
    "        \n",
    "        self.generator = ImageGenerator(folder_path)\n",
    "        self.processor = ImageProcessor()\n",
    "        self.latent_compressor = ImageTargetLatentCompressor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.generator.generate_random_image()\n",
    "        \n",
    "        return self.processor.pil_to_tensor(img), self.latent_compressor.image_to_latent(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d08ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module = loss,\n",
    "        optimizer_class=optim,\n",
    "        scheduler_class=scheduler,\n",
    "        lr: float = lr,\n",
    "        epochs: int = EPOCHS,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        device: torch.device = run_device,\n",
    "        writer: SummaryWriter = SummaryWriter(log_dir=TENSORBOARD_LOG_DIR)\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.writer = writer\n",
    "\n",
    "        self.optimizer = optimizer_class(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = scheduler_class(self.optimizer, T_max=epochs) if scheduler_class else None\n",
    "\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        self.losses = []\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for batch in self.dataloader:\n",
    "                x, z_target = batch  # x = image tensor, z_target = handcrafted latent\n",
    "                x = x.to(self.device)\n",
    "                z_target = z_target.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                z = self.model.encode(x)  # predicted latent\n",
    "                loss = self.criterion(z, z_target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            self.losses.append(avg_loss)\n",
    "\n",
    "            # Log loss and LR\n",
    "            self.writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "            current_lr = self.scheduler.get_last_lr()[0] if self.scheduler else self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar(\"LearningRate\", current_lr, epoch)\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # Log input/reconstruction from full autoencoder pass\n",
    "            with torch.no_grad():\n",
    "                x_sample, _ = next(iter(self.dataloader))\n",
    "                x_sample = x_sample[:8].to(self.device)\n",
    "                z_sample = self.model.encode(x_sample)\n",
    "                x_recon = self.model.decode(z_sample)\n",
    "\n",
    "                self.writer.add_images(\"Input\", x_sample.clamp(0, 1), epoch)\n",
    "                self.writer.add_images(\"Reconstruction\", x_recon.clamp(0, 1), epoch)\n",
    "\n",
    "        self.writer.close()\n",
    "        self._plot_losses()\n",
    "\n",
    "    def _plot_losses(self):\n",
    "        plt.plot(self.losses, marker=\"o\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de99e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=BOTTLENECK_DIM):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),   # 64x64 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),           # -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),          # -> 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),          # -> 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(512),\n",
    "        )\n",
    "        self.encoder_fc = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "            ResidualBlock(512),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1), # -> 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(256),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), # -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(128),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # -> 32x32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.ConvTranspose2d(64, in_channels, 4, 2, 1),  # -> 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.encoder_fc(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c28632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder(latent_dim=BOTTLENECK_DIM).to(run_device)\n",
    "\n",
    "dataset = RandomImageDataset(\"rand_img_components\", 1800)\n",
    "trainer = Trainer(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a48301",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x32768 and 8192x784)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m z_target = z_target.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# predicted latent\u001b[39;00m\n\u001b[32m     42\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(z, z_target)\n\u001b[32m     43\u001b[39m loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mConvAutoencoder.encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m x = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m     70\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\condavnv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (128x32768 and 8192x784)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_proc = ImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_proc.tensor_to_pil(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to file\n",
    "torch.save(model, \"autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsr = torch.randn(768, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PIL from file and reshape to RGB 128x128 (single use)\n",
    "test_img = Image.open(\"test.png\").convert(\"RGB\").resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.zeros(2560, device=run_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test_img thru autoencoder and display:\n",
    "encoded = model.encode(img_proc.pil_to_tensor(test_img).unsqueeze(0).to(run_device))\n",
    "noise[random.randint(0, 767)] += random.randint(-10, 10) * 0.6  # set first dimension to 1\n",
    "encoded = encoded + noise * 0\n",
    "decoded = model.decode(encoded).squeeze(0).clamp(0, 1)\n",
    "img_proc.tensor_to_pil(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condavnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
