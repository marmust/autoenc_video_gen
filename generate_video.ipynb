{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08006c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Model, GPT2LMHeadModel\n",
    "import imageio, os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48baee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "CONVERTED_FRAMERATE = 16\n",
    "\n",
    "WINDOW_SIZE = 24\n",
    "INPUT_WINDOW_SIZE = WINDOW_SIZE - 1\n",
    "ENCODED_DIM = 1200\n",
    "NUM_TRANSFORMER_BLOCKS = 6\n",
    "MLP_HIDDEN_DIM = 2000\n",
    "NUM_HEADS = 12\n",
    "\n",
    "DROPOUT = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1273cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRANSFORMER ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de12fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert ENCODED_DIM % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = ENCODED_DIM // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(ENCODED_DIM, 3 * ENCODED_DIM)\n",
    "        self.out_proj = nn.Linear(ENCODED_DIM, ENCODED_DIM)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, T, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if attn_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa52706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, mlp_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.attn = CausalSelfAttention(num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(ENCODED_DIM, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, ENCODED_DIM),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln1(x), attn_mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3a27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len=INPUT_WINDOW_SIZE, num_heads=NUM_HEADS, mlp_hidden_dim=MLP_HIDDEN_DIM, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(NUM_TRANSFORMER_BLOCKS)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(ENCODED_DIM)\n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        attn_mask = self.causal_mask[:, :, :T, :T]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask)\n",
    "\n",
    "        return self.ln_f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d4ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AUTOENCODER ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352e6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b173c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=ENCODED_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 4, 2, 1),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),           # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),          # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),         # 8x8\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
    "\n",
    "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, enc_out.shape[1:]),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.encoder_fc(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803ca0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MISC ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732a7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def tensor_to_pil(self, image_tensor: torch.Tensor) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert a tensor to a PIL Image.\n",
    "        \n",
    "        Args:\n",
    "            image_tensor (torch.Tensor): A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \n",
    "        Returns:\n",
    "            Image.Image: A PIL Image object.\n",
    "        \"\"\"\n",
    "        # Clamp to [0, 1], convert to [0, 255] and uint8\n",
    "        image_np = (image_tensor.clamp(0, 1).mul(255).byte().cpu().permute(1, 2, 0).numpy())\n",
    "        return Image.fromarray(image_np)\n",
    "    \n",
    "    def pil_to_tensor(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a PIL image to a PyTorch tensor of shape (C, H, W) with values in [0, 1].\n",
    "        \n",
    "        Args:\n",
    "            image (Image.Image): A PIL Image object.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \"\"\"\n",
    "        return transforms.ToTensor()(image)  # Already returns (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59577264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "autoenced_state_dict = torch.load(\"checkpoints/run4/autoencoder.pth\", map_location=run_device)\n",
    "autoencoder.load_state_dict(autoenced_state_dict)\n",
    "autoencoder = autoencoder.to(run_device).eval()\n",
    "\n",
    "# load the transformer\n",
    "transformer = Transformer()\n",
    "transformer_state_dict = torch.load(\"checkpoints/run4/transformer.pth\", map_location=run_device)\n",
    "transformer.load_state_dict(transformer_state_dict)\n",
    "transformer = transformer.to(run_device).eval()\n",
    "\n",
    "# Load image processor\n",
    "proc = ImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1: custom transformer doesnt seem to improve results by much\n",
    "2: noticed that numbers coming from decoder were astronomical, look into activ funcs\n",
    "3: vgg might not be so good after all, consider MSE only training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(num_frames, context_length=INPUT_WINDOW_SIZE, autoencoder=autoencoder, transformer=transformer):\n",
    "    autoencoder.eval()\n",
    "    transformer.eval()\n",
    "    \n",
    "    total_seq = torch.zeros(num_frames + context_length, CHANNELS, RESOLUTION_HEIGHT, RESOLUTION_WIDTH, device=run_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_frames)):\n",
    "            current_slice = total_seq[i:i + context_length]\n",
    "            \n",
    "            slice_latents = autoencoder.encode(current_slice)\n",
    "            \n",
    "            slice_latents += torch.rand(slice_latents.shape, device=run_device) * 99999\n",
    "            \n",
    "            prediction_latents = transformer(slice_latents.unsqueeze(0))\n",
    "            \n",
    "            prediction_frame = autoencoder.decode(prediction_latents.squeeze(0))  # shape: (context_length, C, H, W)\n",
    "            \n",
    "            total_seq[context_length + i] = prediction_frame[-1]\n",
    "    \n",
    "    return total_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames, output='output.mp4', fps=60):\n",
    "    writer = imageio.get_writer(output, fps=fps)\n",
    "    for frame in tqdm(frames):\n",
    "        img = frame.permute(1,2,0).numpy()\n",
    "        img = ((img + 1)/2 * 255).astype('uint8')\n",
    "        writer.append_data(img)\n",
    "    writer.close()\n",
    "    print(f'Saved {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae1884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:06<00:00, 78.59it/s]\n",
      "100%|██████████| 535/535 [00:00<00:00, 2609.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init_img = proc.pil_to_tensor(Image.open(\"test.png\").convert('RGB').resize((RESOLUTION_WIDTH, RESOLUTION_HEIGHT))).to(run_device)\n",
    "\n",
    "frames = generate_frames(512).detach().cpu()\n",
    "torch.cuda.empty_cache()\n",
    "save_video(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([535, 3, 128, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"test.png\").convert('RGB').resize((RESOLUTION_WIDTH, RESOLUTION_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_img = proc.pil_to_tensor(img).to(run_device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoenc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57340\\3275334760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlatent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoenc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'autoenc' is not defined"
     ]
    }
   ],
   "source": [
    "latent = autoenc.encode(enc_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = transformer(inputs_embeds=latent).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = autoenc.decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.tensor_to_pil(decoded.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4941, 0.4980, 0.4980,  ..., 0.4157, 0.4157, 0.4157],\n",
       "         [0.4980, 0.5020, 0.4980,  ..., 0.4196, 0.4196, 0.4196],\n",
       "         [0.4980, 0.5020, 0.5059,  ..., 0.4196, 0.4157, 0.4196],\n",
       "         ...,\n",
       "         [0.4314, 0.3804, 0.4667,  ..., 0.2784, 0.3804, 0.3686],\n",
       "         [0.6471, 0.6235, 0.5843,  ..., 0.2627, 0.2706, 0.2196],\n",
       "         [0.4588, 0.4196, 0.2902,  ..., 0.3020, 0.2784, 0.2784]],\n",
       "\n",
       "        [[0.6745, 0.6745, 0.6784,  ..., 0.6196, 0.6196, 0.6235],\n",
       "         [0.6784, 0.6824, 0.6784,  ..., 0.6275, 0.6275, 0.6275],\n",
       "         [0.6784, 0.6824, 0.6863,  ..., 0.6275, 0.6235, 0.6275],\n",
       "         ...,\n",
       "         [0.4510, 0.4039, 0.4980,  ..., 0.3294, 0.4275, 0.4039],\n",
       "         [0.6863, 0.6706, 0.6353,  ..., 0.3216, 0.3255, 0.2706],\n",
       "         [0.5137, 0.4667, 0.3176,  ..., 0.3765, 0.3490, 0.3529]],\n",
       "\n",
       "        [[0.8392, 0.8392, 0.8392,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.8353, 0.8353, 0.8392,  ..., 0.8431, 0.8392, 0.8353],\n",
       "         [0.8353, 0.8392, 0.8431,  ..., 0.8392, 0.8392, 0.8353],\n",
       "         ...,\n",
       "         [0.1059, 0.0706, 0.1333,  ..., 0.1294, 0.2039, 0.1765],\n",
       "         [0.1686, 0.1569, 0.1686,  ..., 0.1294, 0.1255, 0.0588],\n",
       "         [0.0863, 0.0784, 0.0667,  ..., 0.1686, 0.1490, 0.1333]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
