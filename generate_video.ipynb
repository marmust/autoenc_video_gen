{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08006c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Model, GPT2LMHeadModel\n",
    "import imageio, os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "run_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48baee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video settings\n",
    "RESOLUTION_WIDTH = 128\n",
    "RESOLUTION_HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "CONVERTED_FRAMERATE = 24\n",
    "\n",
    "# model settings\n",
    "WINDOW_SIZE = 48\n",
    "ENCODED_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352e6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b173c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels=CHANNELS, latent_dim=ENCODED_DIM, input_resolution=(RESOLUTION_WIDTH, RESOLUTION_HEIGHT)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 4, 2, 1),  # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),           # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),          # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),         # 8x8\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, *input_resolution)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.flattened_size = enc_out.view(1, -1).shape[1]\n",
    "\n",
    "        self.encoder_fc = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.decoder_fc = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, enc_out.shape[1:]),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x32\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, 4, 2, 1),  # 128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.gelu(self.encoder_fc(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.gelu(self.decoder_fc(z))\n",
    "        z = torch.sigmoid(self.decoder(z))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732a7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def tensor_to_pil(self, image_tensor: torch.Tensor) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert a tensor to a PIL Image.\n",
    "        \n",
    "        Args:\n",
    "            image_tensor (torch.Tensor): A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \n",
    "        Returns:\n",
    "            Image.Image: A PIL Image object.\n",
    "        \"\"\"\n",
    "        # Clamp to [0, 1], convert to [0, 255] and uint8\n",
    "        image_np = (image_tensor.clamp(0, 1).mul(255).byte().cpu().permute(1, 2, 0).numpy())\n",
    "        return Image.fromarray(image_np)\n",
    "    \n",
    "    def pil_to_tensor(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert a PIL image to a PyTorch tensor of shape (C, H, W) with values in [0, 1].\n",
    "        \n",
    "        Args:\n",
    "            image (Image.Image): A PIL Image object.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (C, H, W) with pixel values in the range [0, 1].\n",
    "        \"\"\"\n",
    "        return transforms.ToTensor()(image)  # Already returns (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59577264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder\n",
    "autoencoder = ConvAutoencoder()\n",
    "autoenced_state_dict = torch.load(\"checkpoints/run19/autoenc.pth\", map_location=run_device)\n",
    "autoencoder.load_state_dict(autoenced_state_dict)\n",
    "autoencoder = autoencoder.to(run_device).eval()\n",
    "\n",
    "# load the transformer\n",
    "transformer = GPT2LMHeadModel.from_pretrained(\"checkpoints/run19/gpt2_decap\").transformer\n",
    "transformer = transformer.to(run_device).eval()\n",
    "\n",
    "# Load image processor\n",
    "proc = ImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(num_frames, context_length=WINDOW_SIZE, autoencoder=autoencoder, transformer=transformer):\n",
    "    autoencoder.eval()\n",
    "    transformer.eval()\n",
    "    \n",
    "    total_seq = torch.zeros(num_frames + context_length, CHANNELS, RESOLUTION_HEIGHT, RESOLUTION_WIDTH, device=run_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_frames)):\n",
    "            current_slice = total_seq[i:i + context_length]\n",
    "            \n",
    "            slice_latents = autoencoder.encode(current_slice)\n",
    "            \n",
    "            slice_latents += torch.rand(slice_latents.shape, device=run_device) * 999\n",
    "            \n",
    "            prediction_latents = transformer(inputs_embeds=slice_latents.unsqueeze(0)).last_hidden_state\n",
    "            \n",
    "            prediction_frame = autoencoder.decode(prediction_latents.squeeze(0))  # shape: (context_length, C, H, W)\n",
    "            \n",
    "            total_seq[context_length + i] = prediction_frame[-1]\n",
    "    \n",
    "    return total_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames, output='output.mp4', fps=60):\n",
    "    writer = imageio.get_writer(output, fps=fps)\n",
    "    for frame in tqdm(frames):\n",
    "        img = frame.permute(1,2,0).numpy()\n",
    "        img = ((img + 1)/2 * 255).astype('uint8')\n",
    "        writer.append_data(img)\n",
    "    writer.close()\n",
    "    print(f'Saved {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae1884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:10<00:00, 50.62it/s]\n",
      "100%|██████████| 560/560 [00:00<00:00, 2864.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init_img = proc.pil_to_tensor(Image.open(\"test.png\").convert('RGB').resize((RESOLUTION_WIDTH, RESOLUTION_HEIGHT))).to(run_device)\n",
    "\n",
    "frames = generate_frames(512).detach().cpu()\n",
    "torch.cuda.empty_cache()\n",
    "save_video(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([560, 3, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"test.png\").convert('RGB').resize((RESOLUTION_WIDTH, RESOLUTION_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_img = proc.pil_to_tensor(img).to(run_device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoenc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m latent = \u001b[43mautoenc\u001b[49m.encode(enc_img)\n",
      "\u001b[31mNameError\u001b[39m: name 'autoenc' is not defined"
     ]
    }
   ],
   "source": [
    "latent = autoenc.encode(enc_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = transformer(inputs_embeds=latent).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = autoenc.decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.tensor_to_pil(decoded.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4941, 0.4980, 0.4980,  ..., 0.4157, 0.4157, 0.4157],\n",
       "         [0.4980, 0.5020, 0.4980,  ..., 0.4196, 0.4196, 0.4196],\n",
       "         [0.4980, 0.5020, 0.5059,  ..., 0.4196, 0.4157, 0.4196],\n",
       "         ...,\n",
       "         [0.4314, 0.3804, 0.4667,  ..., 0.2784, 0.3804, 0.3686],\n",
       "         [0.6471, 0.6235, 0.5843,  ..., 0.2627, 0.2706, 0.2196],\n",
       "         [0.4588, 0.4196, 0.2902,  ..., 0.3020, 0.2784, 0.2784]],\n",
       "\n",
       "        [[0.6745, 0.6745, 0.6784,  ..., 0.6196, 0.6196, 0.6235],\n",
       "         [0.6784, 0.6824, 0.6784,  ..., 0.6275, 0.6275, 0.6275],\n",
       "         [0.6784, 0.6824, 0.6863,  ..., 0.6275, 0.6235, 0.6275],\n",
       "         ...,\n",
       "         [0.4510, 0.4039, 0.4980,  ..., 0.3294, 0.4275, 0.4039],\n",
       "         [0.6863, 0.6706, 0.6353,  ..., 0.3216, 0.3255, 0.2706],\n",
       "         [0.5137, 0.4667, 0.3176,  ..., 0.3765, 0.3490, 0.3529]],\n",
       "\n",
       "        [[0.8392, 0.8392, 0.8392,  ..., 0.8392, 0.8353, 0.8314],\n",
       "         [0.8353, 0.8353, 0.8392,  ..., 0.8431, 0.8392, 0.8353],\n",
       "         [0.8353, 0.8392, 0.8431,  ..., 0.8392, 0.8392, 0.8353],\n",
       "         ...,\n",
       "         [0.1059, 0.0706, 0.1333,  ..., 0.1294, 0.2039, 0.1765],\n",
       "         [0.1686, 0.1569, 0.1686,  ..., 0.1294, 0.1255, 0.0588],\n",
       "         [0.0863, 0.0784, 0.0667,  ..., 0.1686, 0.1490, 0.1333]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condavnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
